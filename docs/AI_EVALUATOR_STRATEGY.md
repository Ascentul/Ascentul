# Ascentful AI Evaluator Model Strategy

*Last updated: December 2025*

## 0. Review Council and Ownership

Evaluator strategy and changes are governed by the **AI Evaluation Governance Council**.

### Members

- Head of AI and Prompt Quality (chair)
- Head of Product
- Head of Security and Privacy
- Head of Compliance
- Data and Analytics Lead
- Head of Engineering

### Responsibilities

- Approve any changes to:
  - Evaluator model choice
  - Rubric structure and scoring
  - Thresholds that decide what students and advisors see
- Ensure evaluation is aligned with:
  - Student protection and well being
  - Advisor workflows and expectations
  - University policies and FERPA constraints
- Require calibration runs on golden datasets before rollout of a new evaluator version to pilot or production.
- Review incident reports where evaluator behavior created risk or required manual intervention.

---

## 1. Decision Summary

**Strategy:** Use a **hybrid approach**:

- A single primary **evaluator model** that acts as the model judge for quality, alignment, tone, and rubric-based scoring.
- A layer of **deterministic rules and validators** that run before and after the model to enforce safety, schema correctness, and hard constraints.

The core decision is:

> Use one evaluator model for all tools, paired with strict rule-based checks. Do not manage multiple competing evaluator models per tool during pilots.

This keeps behavior consistent and auditable, while still giving strong guardrails around student and advisor data.

---

## 2. Scope

The evaluator strategy applies to any feature where Ascentful uses AI to generate or transform content and then judge it, including:

### Tools Requiring Evaluation

| Tool ID | Description | Priority |
|---------|-------------|----------|
| `resume-generation` | AI-generated resumes | HIGH |
| `resume-analysis` | Resume vs job description analysis | MEDIUM |
| `resume-optimization` | AI-optimized resume content | HIGH |
| `resume-suggestions` | Quick improvement suggestions | MEDIUM |
| `resume-parse` | Resume text extraction | LOW |
| `cover-letter-generation` | AI-generated cover letters | HIGH |
| `cover-letter-analysis` | Cover letter quality analysis | MEDIUM |
| `ai-coach-response` | Career coach responses | HIGH |
| `ai-coach-message` | Coach conversation messages | HIGH |
| `career-path-generation` | Career progression paths | MEDIUM |
| `career-path-from-job` | Path from job description | MEDIUM |
| `career-paths-generation` | Multiple path options | MEDIUM |
| `career-certifications` | Certification recommendations | MEDIUM |

For these tools, evaluation provides:

- A structured score per rubric dimension
- Flags for risky or non-compliant output
- Short natural language rationales

---

## 3. Components of the Strategy

### 3.1 Single Evaluator Model

Use one main evaluator model for all tools, configured differently per tool through prompts and rubrics.

**Characteristics**

- Reasoning-focused rather than pure generation-focused
- Strong at following step-by-step instructions
- Deterministic settings where possible (low temperature, stable prompts)

**Current Configuration**

```typescript
{
  evaluator_model: 'gpt-4o-mini',
  temperature: 0.1,
  enabled: true,
  log_all: true,
  block_on_fail: false,
}
```

**Responsibilities**

The evaluator model:

- Reads the tool output, the input context, and the rubric
- Produces:
  - `overall_score` (0-100)
  - `dimension_scores` per rubric dimension
  - `risk_flags` for specific concerns
  - `explanation` for audit and debugging

The evaluator model does **not** generate student-facing content. It judges content generated by other tools or models.

### 3.2 Rule-Based Checks

Rule-based checks sit alongside the evaluator model and handle things that should not rely on a language model.

**Use Rules For**

- **JSON schema validation** (Zod)
  - Required keys present
  - Correct field types
  - Allowed ranges for numeric values
- **Length limits**
  - Maximum lengths for certain fields
  - Reasonable bounds for lists and arrays
- **Forbidden patterns**
  - Direct copying of long segments from a job description
  - Inclusion of other people's private contact details
  - URLs outside an allowed set
- **Safety and compliance constraints**
  - No discriminatory or unethical guidance
  - No advice that conflicts with university career center ethics

**Execution Points**

Rules run:

1. **Pre-evaluation**
   - Reject or clean obviously invalid outputs
   - Example: wrong JSON shape, empty required sections
2. **Post-evaluation**
   - Enforce hard constraints even if the model misses something
   - Example: block content that contains a forbidden pattern even if the evaluator rated it highly

If any hard rule fails, the result is tagged as invalid or high risk without relying on the evaluator model's score.

### 3.3 Tool-Specific Rubrics

Each AI-powered feature has its own rubric file that defines:

- Scored dimensions (e.g., Relevance, Impact, Clarity, ATS Readiness)
- Weight for each dimension (must sum to 1.0)
- What qualifies as Excellent, Acceptable, Needs Improvement, Unacceptable
- Pass threshold (typically 65-70)
- Critical risk flags that cause automatic failure

Rubrics are:

- Stored in `src/lib/ai-evaluation/rubrics/`
- Versioned and tracked in git
- Not hard-coded inside evaluator logic

---

## 4. Why Not Multiple Evaluator Models

Alternative considered: Using different evaluator models per tool or per use case.

**Reasons to Avoid This for Pilots**

- Harder to compare scores across tools and over time
- More complex to audit and explain to universities and IT
- Increased operational overhead to monitor and update multiple evaluators
- Higher risk of inconsistent behavior under stress or incident conditions

**Pilot and Early Production Policy**

- One evaluator model
- Many tool-specific rubrics and rule sets

If later a new evaluator model is needed for cost or performance, it is treated as a **new evaluator version** behind an abstraction, not as a different evaluator per tool.

---

## 5. Evaluation Pipeline

Standard evaluation flow for any AI tool:

```
┌─────────────────────────────────────────────────────────────────┐
│                    AI Tool (e.g., Resume Generator)             │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│              PRE-EVALUATION RULE CHECKS                         │
│  • JSON schema validation (Zod)                                 │
│  • Length limits                                                │
│  • Required fields present                                      │
│  • Forbidden pattern detection                                  │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                    EVALUATOR SERVICE                            │
│  • Single model (GPT-4o-mini)                                   │
│  • Tool-specific rubric loaded from config                      │
│  • Returns: overall_score, dimension_scores, risk_flags         │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│              POST-EVALUATION RULE CHECKS                        │
│  • Threshold enforcement                                        │
│  • Hard constraint validation                                   │
│  • Risk flag blocking                                           │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                    PERSIST & RESPOND                            │
│  • Store in ai_evaluations table                                │
│  • Return to user or trigger regeneration                       │
└─────────────────────────────────────────────────────────────────┘
```

### Detailed Steps

1. **Tool generates a candidate output**
   - Example: resume suggestions, interview feedback, career recommendations

2. **Pre-evaluation rule checks**
   - Validate JSON schema and required fields
   - Check length limits and simple forbidden patterns
   - If this fails: Mark result as invalid, optionally regenerate

3. **Evaluator model call**
   - Input: Tool name, rubric definition, context, candidate output
   - Output: `overall_score`, `dimension_scores`, `risk_flags`, `explanation`

4. **Post-evaluation rule checks**
   - Apply threshold rules (if `overall_score` below minimum, mark as failing)
   - Apply critical flag rules (if any critical flags, mark as high risk)
   - Apply tool-specific blocking rules

5. **Persist evaluation result**
   - Store in `ai_evaluations` Convex table
   - Include: tool ID, scores, flags, timestamps, environment

6. **Use evaluation outcome**
   - For student/advisor flows: Only show content that passes thresholds
   - For analytics: Use data to improve prompts, rubrics, and thresholds

---

## 6. Risk Flags

### Available Flags

| Flag | Description | Typical Severity |
|------|-------------|------------------|
| `too_generic` | Content lacks specificity | Warning |
| `potential_bias` | May contain biased language | Critical |
| `missing_requirements` | Doesn't address key requirements | Warning |
| `factual_inconsistency` | Claims don't match profile | Critical |
| `pii_detected` | Contains PII | Critical |
| `discriminatory_content` | Discriminatory language | Critical |
| `hallucination_detected` | Made up facts | Critical |
| `low_relevance` | Not relevant to context | Warning |
| `unprofessional_tone` | Inappropriate language | Warning |
| `excessive_length` | Output too long | Warning |
| `insufficient_detail` | Output too short | Warning |
| `safety_concern` | Could harm user | Critical |
| `out_of_scope` | Outside career guidance | Warning |
| `copy_paste_detected` | Copied from input | Warning |

### Critical Flags by Tool

- **Resume tools**: `factual_inconsistency`, `pii_detected`, `hallucination_detected`
- **AI Coach**: `discriminatory_content`, `potential_bias`, `safety_concern`, `out_of_scope`
- **Cover Letter**: `too_generic`, `factual_inconsistency`, `hallucination_detected`
- **Career Path**: `hallucination_detected`, `low_relevance`

---

## 7. Calibration and Review

To keep the evaluator reliable and aligned:

### Golden Datasets

- Create small but high-quality labeled sets per tool
- Each item has: Input context, model output, human-assigned scores and risk labels

### Calibration Runs

- Periodically run the evaluator model on golden datasets
- Compare evaluator scores against human labels
- Adjust rubrics, thresholds, and prompt instructions when drift is detected

### Change Control

Any change to:
- Evaluator model identity or version
- Rubric structures
- Thresholds that affect pass/fail decisions

Must:
- Be reviewed by the AI Evaluation Governance Council
- Include a brief calibration run against golden datasets
- Be documented in an evaluation change log

---

## 8. Environments and Safety

Evaluator behavior follows Ascentful's environment strategy.

### Development

- Synthetic or anonymized data only
- Open to experimentation and debugging
- Relaxed thresholds, no real student or advisor impact

### Staging

- Near-production configuration
- Synthetic or anonymized data, but realistic shapes
- Used to test evaluator changes, rubrics, and pipelines before pilots

### Production

- Only approved evaluator configurations and rubrics
- Changes go through: Council review, calibration, staging validation
- Evaluation results kept for audit and continuous improvement

---

## 9. Database Schema

### ai_evaluations Table

Stores all evaluation results:

```typescript
{
  tool_id: string,
  tool_version?: string,
  evaluator_model: string,
  rubric_version: string,
  overall_score: number,      // 0-100
  dimension_scores: object,   // Record<string, DimensionScore>
  risk_flags: string[],
  explanation: string,
  passed: boolean,
  user_id?: string,
  input_hash: string,
  output_hash: string,
  environment: 'dev' | 'staging' | 'production',
  evaluation_duration_ms: number,
  created_at: number,
}
```

### ai_evaluation_config Table

Per-tool configuration overrides:

```typescript
{
  tool_id: string,
  rubric?: object,           // Override default rubric
  pass_threshold?: number,   // Override threshold
  enabled: boolean,
  block_on_fail: boolean,
  updated_at: number,
  updated_by: string,
}
```

---

## 10. Usage

### Basic Usage

```typescript
import { evaluate } from '@/lib/ai-evaluation';

const result = await evaluate({
  tool_id: 'resume-generation',
  output: generatedResume,
  input_context: { jobDescription, userProfile },
  user_id: userId,
});

if (!result.passed) {
  console.log('Evaluation failed:', result.explanation);
  console.log('Risk flags:', result.risk_flags);
}
```

### With Convex Storage

```typescript
import { useMutation } from 'convex/react';
import { api } from 'convex/_generated/api';

const createEvaluation = useMutation(api.ai_evaluations.createEvaluation);

await createEvaluation({
  tool_id: result.tool_id,
  overall_score: result.overall_score,
  dimension_scores: result.dimension_scores,
  risk_flags: result.risk_flags,
  // ... other fields
});
```

---

## 11. Operational Ownership

### Primary Operational Owner

- Head of AI and Prompt Quality

### Technical Owner

- Lead Backend Engineer (evaluator service implementation)

### Stakeholders

- Head of Product
- Head of Security and Privacy
- Data and Analytics Lead
- Customer Success Lead

### Responsibilities

- Maintain evaluator prompts, rubrics, and thresholds
- Coordinate calibration runs and golden dataset reviews
- Monitor evaluator metrics (score distribution, flag frequency, rejection rates)
- Document significant changes affecting pilot users
- Raise incidents when evaluator behavior creates safety or trust concerns

---

## 12. File Structure

```
src/lib/ai-evaluation/
├── index.ts                    # Main exports
├── types.ts                    # TypeScript types
├── evaluator.ts                # Evaluator service
├── rubrics/
│   ├── index.ts                # Rubric registry
│   ├── resume.ts               # Resume tool rubrics
│   ├── cover-letter.ts         # Cover letter rubrics
│   ├── ai-coach.ts             # AI coach rubrics
│   ├── career-path.ts          # Career path rubrics
│   └── career-certifications.ts
├── rules/
│   ├── index.ts                # Rule engine
│   └── forbidden-patterns.ts   # Safety patterns
└── schemas/
    └── index.ts                # Zod output schemas

convex/
├── schema.ts                   # ai_evaluations, ai_evaluation_config tables
└── ai_evaluations.ts           # Convex queries and mutations

docs/
└── AI_EVALUATOR_STRATEGY.md    # This document
```

---

## 13. Success Metrics

| Metric | Target |
|--------|--------|
| Coverage | 100% of AI tools have evaluation |
| Pass Rate | >85% in production |
| Latency | <500ms additional per evaluation |
| Risk Detection | <1% false negative on critical flags |
